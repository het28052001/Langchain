{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3318885863f54d8082a42d6368a216ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_112f1d07e94c4d7aafe9de8a94e916cf",
              "IPY_MODEL_7e1e7b2b19624724a41333ccdeec5a75",
              "IPY_MODEL_9cd8c83861a944c3a68f42ee5c092ed1"
            ],
            "layout": "IPY_MODEL_06d9e5e89c864948818b7c1ec3fe1f25"
          }
        },
        "112f1d07e94c4d7aafe9de8a94e916cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1b2a56b96f7425e8d3d7af440d2f879",
            "placeholder": "​",
            "style": "IPY_MODEL_db9fec22f33945098aa34801d0913c05",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7e1e7b2b19624724a41333ccdeec5a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcbaaa8f4bc54e20a718ba2fe4731c4d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_154975da07fc4f7085daa31ead5e53ba",
            "value": 2
          }
        },
        "9cd8c83861a944c3a68f42ee5c092ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_805f131bbc76408bb8a38e8c9ebbd373",
            "placeholder": "​",
            "style": "IPY_MODEL_e5b7c31c70a44527be7e7287c730d054",
            "value": " 2/2 [01:03&lt;00:00, 29.27s/it]"
          }
        },
        "06d9e5e89c864948818b7c1ec3fe1f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b2a56b96f7425e8d3d7af440d2f879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9fec22f33945098aa34801d0913c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcbaaa8f4bc54e20a718ba2fe4731c4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "154975da07fc4f7085daa31ead5e53ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "805f131bbc76408bb8a38e8c9ebbd373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5b7c31c70a44527be7e7287c730d054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aMV5NcU4awjz"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    transformers==4.31.0 \\\n",
        "    accelerate==0.21.0 \\\n",
        "    einops==0.6.1 \\\n",
        "    langchain==0.0.240 \\\n",
        "    xformers==0.0.20 \\\n",
        "    bitsandbytes==0.41.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "# model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "3318885863f54d8082a42d6368a216ac",
            "112f1d07e94c4d7aafe9de8a94e916cf",
            "7e1e7b2b19624724a41333ccdeec5a75",
            "9cd8c83861a944c3a68f42ee5c092ed1",
            "06d9e5e89c864948818b7c1ec3fe1f25",
            "d1b2a56b96f7425e8d3d7af440d2f879",
            "db9fec22f33945098aa34801d0913c05",
            "dcbaaa8f4bc54e20a718ba2fe4731c4d",
            "154975da07fc4f7085daa31ead5e53ba",
            "805f131bbc76408bb8a38e8c9ebbd373",
            "e5b7c31c70a44527be7e7287c730d054"
          ]
        },
        "id": "0DJ94KrJbDql",
        "outputId": "5e8b5d4d-839c-4a26-970f-1d47dd7afd17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3318885863f54d8082a42d6368a216ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CrQiQ3ab5Hq",
        "outputId": "bb6f9bc9-6147-4652-c354-8c6ebb0bda2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=4048,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "f2rE4jgnmxsE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing purpose"
      ],
      "metadata": {
        "id": "JRZ_haZZpJiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"What is the capital of France\")\n",
        "print(res[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1KuqF1tm1ZB",
        "outputId": "d19a5762-25b7-421f-c612-1fa0b80b49a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of France?\n",
            " Unterscheidung zwischen \"Capital\" und \"Hauptstadt\" – Welche Bezeichnung ist richtig?\n",
            "What is the capital of France?\n",
            "\n",
            "A: The capital of France is Paris.\n",
            "\n",
            "B: The capital of France is Berlin.\n",
            "\n",
            "C: The capital of France is London.\n",
            "\n",
            "D: The capital of France is Madrid.\n",
            "\n",
            "Correct answer: A - The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
        "print(res[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxigHjqho8g-",
        "outputId": "62539f12-b593-44e8-eb1b-5a5fc25d692f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain to me the difference between nuclear fission and fusion. Unterscheidung zwischen Nuklearfusion und -fission.\n",
            "Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. This is typically achieved through the use of neutron bombardment, where a nucleus is struck by a neutron, causing it to become unstable and split into two or more smaller nuclei. Fission reactions are typically used in nuclear reactors to generate electricity, as the released energy can be harnessed to produce steam, which then drives a turbine to generate electricity.\n",
            "Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases energy, but in this case, the energy is produced through the binding of the nuclei together. Fusion reactions are typically used in the sun and other stars, where they are responsible for producing the light and heat that we see.\n",
            "The main difference between nuclear fission and fusion is the direction of the energy release. In fission, the energy is released in the form of kinetic energy of the fragments, while in fusion, the energy is released in the form of electromagnetic radiation (such as light and heat). Additionally, fission reactions typically involve the splitting of a heavy nucleus into two or more lighter nuclei, while fusion reactions involve the combination of two or more light nuclei into a single, heavier nucleus.\n",
            "In summary, nuclear fission is the process of splitting a heavy nucleus into two or more lighter nuclei, releasing energy in the process, while nuclear fusion is the process of combining two or more light nuclei into a single, heavier nucleus, also releasing energy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementing with Langchain"
      ],
      "metadata": {
        "id": "GmqA9DXMpPha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ],
      "metadata": {
        "id": "yEe5cqO6oR_a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"Explain to me the difference between nuclear fission and fusion.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "5RU5-_hApb8v",
        "outputId": "ecb40733-36f3-4659-efde-e4571af40690"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Unterscheidung zwischen Nuklearfusion und -fission.\\nNuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. This is typically achieved through the use of neutron bombardment, where a nucleus is struck by a neutron, causing it to become unstable and split into two or more smaller nuclei. Fission reactions are typically used in nuclear reactors to generate electricity, as the released energy can be harnessed to produce steam, which then drives a turbine to generate electricity.\\nNuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases energy, but in this case, the energy is produced through the binding of the nuclei together. Fusion reactions are typically used in the sun and other stars, where they are responsible for producing the light and heat that we see.\\nThe main difference between nuclear fission and fusion is the direction of the energy release. In fission, the energy is released in the form of kinetic energy of the fragments, while in fusion, the energy is released in the form of electromagnetic radiation (such as light and heat). Additionally, fission reactions typically involve the splitting of a heavy nucleus into two or more lighter nuclei, while fusion reactions involve the combination of two or more light nuclei into a single, heavier nucleus.\\nIn summary, nuclear fission is the process of splitting a heavy nucleus into two or more lighter nuclei, releasing energy in the process, while nuclear fusion is the process of combining two or more light nuclei into a single, heavier nucleus, also releasing energy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"Explain to me about the future of the NVIDIA stock price.\")"
      ],
      "metadata": {
        "id": "6ch_PNiSpefH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"Which team had won the IPL 2021?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "P-NpIIWCpuDm",
        "outputId": "1bde9eaf-8a26-4b4f-ac8e-2f0894010755"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n nobody knows yet. The 2021 Indian Premier League (IPL) is currently in progress, and the teams are competing to win the tournament. The final match of the IPL 2021 will be played on May 30, 2021, and the winner will be declared then.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"What is the capital of France?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lhssvGPwrTOy",
        "outputId": "16dbe304-853d-4ca5-dd86-9be6e05b272d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n Unterscheidung zwischen \"Capital\" und \"Hauptstadt\" – Welche Bezeichnung ist richtig?\\nWhat is the capital of France?\\n\\nA: The capital of France is Paris.\\n\\nB: The capital of France is Berlin.\\n\\nC: The capital of France is London.\\n\\nD: The capital of France is Madrid.\\n\\nCorrect answer: A - The capital of France is Paris.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conversational agent\n"
      ],
      "metadata": {
        "id": "hALCh2FosaVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first need to initialize the agent. Conversational agents require several things such as conversational memory, access to tools, and an llm (which we have already initialized)."
      ],
      "metadata": {
        "id": "kxf3WzrZspUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.agents import load_tools\n",
        "\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
        ")\n",
        "tools = load_tools([\"llm-math\"], llm=llm)"
      ],
      "metadata": {
        "id": "2g_kDuIere-0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentOutputParser\n",
        "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
        "from langchain.output_parsers.json import parse_json_markdown\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "\n",
        "class OutputParser(AgentOutputParser):\n",
        "    def get_format_instructions(self) -> str:\n",
        "        return FORMAT_INSTRUCTIONS\n",
        "\n",
        "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
        "        try:\n",
        "            # this will work IF the text is a valid JSON with action and action_input\n",
        "            response = parse_json_markdown(text)\n",
        "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
        "            if action == \"Final Answer\":\n",
        "                # this means the agent is finished so we call AgentFinish\n",
        "                return AgentFinish({\"output\": action_input}, text)\n",
        "            else:\n",
        "                # otherwise the agent wants to use an action, so we call AgentAction\n",
        "                return AgentAction(action, action_input, text)\n",
        "        except Exception:\n",
        "            # sometimes the agent will return a string that is not a valid JSON\n",
        "            # often this happens when the agent is finished\n",
        "            # so we just return the text as the output\n",
        "            return AgentFinish({\"output\": text}, text)\n",
        "\n",
        "    @property\n",
        "    def _type(self) -> str:\n",
        "        return \"conversational_chat\"\n",
        "\n",
        "# initialize output parser for agent\n",
        "parser = OutputParser()"
      ],
      "metadata": {
        "id": "cr_UAvLgsoLc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# initialize agent\n",
        "agent = initialize_agent(\n",
        "    agent=\"chat-conversational-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    early_stopping_method=\"generate\",\n",
        "    memory=memory,\n",
        "    agent_kwargs={\"output_parser\": parser}\n",
        ")"
      ],
      "metadata": {
        "id": "xKb3w53is4w1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.agent.llm_chain.prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyPl2mWBtFxF",
        "outputId": "3bfa9c58-c1de-4de2-c444-3f09f431f005"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Calculator: Useful for when you need to answer questions about math.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\ The action to take. Must be one of Calculator\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to add special tokens used to signify the beginning and end of instructions, and the beginning and end of system messages. These are described in the Llama-2 model cards on Hugging Face.\n",
        "\n"
      ],
      "metadata": {
        "id": "0gc5LMf-tdDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
      ],
      "metadata": {
        "id": "jcJ8j1XEtQ9n"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_msg = B_SYS + \"\"\"Assistant is a expert JSON builder designed to assist with a wide range of tasks.\n",
        "\n",
        "Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n",
        "\n",
        "All of Assistant's communication is performed using this JSON format.\n",
        "\n",
        "Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\n",
        "\n",
        "- \"Calculator\": Useful for when you need to answer questions about math.\n",
        "  - To use the calculator tool, Assistant should write like so:\n",
        "    ```json\n",
        "    {{\"action\": \"Calculator\",\n",
        "      \"action_input\": \"sqrt(4)\"}}\n",
        "    ```\n",
        "\n",
        "Here are some previous conversations between the Assistant and User:\n",
        "\n",
        "User: Hey how are you today?\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"I'm good thanks, how are you?\"}}\n",
        "```\n",
        "User: I'm great, what is the square root of 4?\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Calculator\",\n",
        " \"action_input\": \"sqrt(4)\"}}\n",
        "```\n",
        "User: 2.0\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"It looks like the answer is 2!\"}}\n",
        "```\n",
        "User: Thanks could you tell me what 4 to the power of 2 is?\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Calculator\",\n",
        " \"action_input\": \"4**2\"}}\n",
        "```\n",
        "User: 16.0\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"It looks like the answer is 16!\"}}\n",
        "```\n",
        "\n",
        "Here is the latest conversation between Assistant and User.\"\"\" + E_SYS\n",
        "new_prompt = agent.agent.create_prompt(\n",
        "    system_message=sys_msg,\n",
        "    tools=tools\n",
        ")\n",
        "agent.agent.llm_chain.prompt = new_prompt"
      ],
      "metadata": {
        "id": "roPiqL5nunpM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = B_INST + \" Respond to the following in JSON with 'action' and 'action_input' values \" + E_INST\n",
        "human_msg = instruction + \"\\nUser: {input}\"\n",
        "\n",
        "agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg"
      ],
      "metadata": {
        "id": "tClrADJOu0Ea"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.agent.llm_chain.prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVdplLLru7pG",
        "outputId": "b8cf98d3-dae3-40eb-9049-de673b070829"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='<<SYS>>\\nAssistant is a expert JSON builder designed to assist with a wide range of tasks.\\n\\nAssistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\\n\\nAll of Assistant\\'s communication is performed using this JSON format.\\n\\nAssistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\\n\\n- \"Calculator\": Useful for when you need to answer questions about math.\\n  - To use the calculator tool, Assistant should write like so:\\n    ```json\\n    {{\"action\": \"Calculator\",\\n      \"action_input\": \"sqrt(4)\"}}\\n    ```\\n\\nHere are some previous conversations between the Assistant and User:\\n\\nUser: Hey how are you today?\\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": \"I\\'m good thanks, how are you?\"}}\\n```\\nUser: I\\'m great, what is the square root of 4?\\nAssistant: ```json\\n{{\"action\": \"Calculator\",\\n \"action_input\": \"sqrt(4)\"}}\\n```\\nUser: 2.0\\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": \"It looks like the answer is 2!\"}}\\n```\\nUser: Thanks could you tell me what 4 to the power of 2 is?\\nAssistant: ```json\\n{{\"action\": \"Calculator\",\\n \"action_input\": \"4**2\"}}\\n```\\nUser: 16.0\\nAssistant: ```json\\n{{\"action\": \"Final Answer\",\\n \"action_input\": \"It looks like the answer is 16!\"}}\\n```\\n\\nHere is the latest conversation between Assistant and User.\\n<</SYS>>\\n\\n', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template=\"[INST] Respond to the following in JSON with 'action' and 'action_input' values [/INST]\\nUser: {input}\", template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will communicate with agents\n"
      ],
      "metadata": {
        "id": "hvuomeqIvGzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"How are you\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH1LTKXnvBS-",
        "outputId": "54aa5a93-29c5-4e55-d0ff-f6c1e136aac5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m today?\n",
            "Assistant: {\n",
            "\"action\": \"Final Answer\",\n",
            "\"action_input\": \"I'm good thanks, how are you?\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'How are you',\n",
              " 'chat_history': [],\n",
              " 'output': ' today?\\nAssistant: {\\n\"action\": \"Final Answer\",\\n\"action_input\": \"I\\'m good thanks, how are you?\"\\n}'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"what is 4 to the power of 2.1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fowgJcfvKgW",
        "outputId": "f783180b-e79f-4b23-b0b8-dabc043fb0e1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Assistant: {\n",
            "\"action\": \"Calculator\",\n",
            "\"action_input\": \"4**2.1\"}\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is 4 to the power of 2.1?',\n",
              " 'chat_history': [HumanMessage(content='How are you', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=' today?\\nAssistant: {\\n\"action\": \"Final Answer\",\\n\"action_input\": \"I\\'m good thanks, how are you?\"\\n}', additional_kwargs={}, example=False)],\n",
              " 'output': '\\nAssistant: {\\n\"action\": \"Calculator\",\\n\"action_input\": \"4**2.1\"}\\n}'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"can you multiply that by 3?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9VDT2LLDNW1",
        "outputId": "4bb1aaaa-8c5a-4d55-c614-39ada1a8ed1f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Assistant: {\n",
            "\"action\": \"Multiply\",\n",
            "\"action_input\": \"3*4\"}\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'can you multiply that by 3?',\n",
              " 'chat_history': [HumanMessage(content='How are you', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=' today?\\nAssistant: {\\n\"action\": \"Final Answer\",\\n\"action_input\": \"I\\'m good thanks, how are you?\"\\n}', additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='what is 4 to the power of 2.1?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='\\nAssistant: {\\n\"action\": \"Calculator\",\\n\"action_input\": \"4**2.1\"}\\n}', additional_kwargs={}, example=False)],\n",
              " 'output': '\\nAssistant: {\\n\"action\": \"Multiply\",\\n\"action_input\": \"3*4\"}\\n}'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2751ikhDbcN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}